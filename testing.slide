Testing en Golang
Metodologías y recomendaciones

Agosto 2017

Ariel Gerardo Ríos

* Contenido

Metodologías:

1. Tradicional
2. Subtests
3. Table driven test
4. Test fixtures
5. Golden files
6. Test flags
7. Test helpers
8. Benchmarks

Recomendaciones:

a. Limitarse a la API pùblica
b. Mocks: ¿son realmente necesarios?
c. Configurabilidad
d. Subprocessing
e. Paralelización

* Metodologías

* 1. Tradicional

    // In package_test.go

    func TestUseCase1(t *testing.T) {
        // set up
        // -- testing
        // tear down
    }

    func TestUseCase2(t *testing.T) {
        // set up
        // -- testing
        // tear down
    }

Ejecución:
    
    $ go test
    $ go test -run TestUseCase1  # single test

* 2. Subtests

Agrupamiento de test unitarios dentro de otro test, idealmente relacionados.

    // In package_test.go

    func TestUnit(t *testing.T) {
        // set up

        t.Run("UseCase1", func (t *testing.T) { /* ... */ }
        
        t.Run("UseCase2", func (t *testing.T) { /* ... */ }

        // tear down
    }

Ejecución:

    $ go test
    $ go test -run TestUnit  # single suite
    $ go test -run TestUnit/UseCase2  # single case!

: Beneficios:
: - Reutilización de objetos con inicialiación costosa.
: - Agrupamiento de test unitarios por objetivo o funcionalidad testeada ( a.k.a. test suite).

* 3. Table driven test

Casos de uso nombrados, con valores de entrada y salida esperados.

    table = {
        CU-1: (input-1, ..., input-n, output)
        ...
        CU-m: (input-1, ..., input-n, output)
    }

Ejemplo:

    func TestFormatter(t *testing.T) {
    	var table = map[string]struct{ A, B, Expected string }{
    		"only-strings": {"a", "b", "a-xx-b"},
    		"with-numbers": {"01", "02", "01-xx-02"},
    	}
    
    	for _, tc := range table {
    		s := somelib.Method(tc.A, tc.B)
    		if s != tc.Expected {
    			t.Errorf("Method(%q, %q) => %q, want %q", tc.A, tc.B, s, tc.Expected)
    		}
    	}
    }

: Descripción:
: Un array formado por uno o más valores de entrada, acompañados por el valor de salida esperado.
: Beneficios:
: - Facilidad para agregar más casos de uso a la lista.
: - Conveniente para reutilizar objetos que son de creación costosa.
: Recomendaciones:
: - Usar nombres en los casos de uso para fácil localización.
: - Implementarlo aún en aquellos tests formados por casos de uso únicos, pero que tengan posibilidad de crecer en el futuro.

* 4. Test fixtures

Extraer datos de inicialización del test para reducir polución.

    // In ./some_test.go
    func TestOperation(t *testing.T) {
        path := filepath.Join("testdata", "fixture.json")
        data, err := ioutil.ReadFile(path)

        // -- then do some verification using the data
    }

    // In ./testdata/fixture.json
    {
        "config_1": 25,
        "config_2": "on",
        "objects": [
            {"id": 1, "name": "Api"},
            {"id": 2, "name": "Core"}
        ]
    }

: A tener en cuenta:
: - Los directorios llamados "testdata" serán ignorados por el compilador; son útiles para esta responsabilidad.
: Beneficios: 
: - Útil para configuraciones, datos de modelos, datos binarios, etc.
: Recomendaciones:
: - Usar paths relativos desde los tests.

* 5. Golden files

Valida la diferencia entre el resultado obtenido y una copia del esperado.

    func TestGolden(t *testing.T) {
    	req, _ := http.NewRequest("GET", "/", nil)
    	w := httptest.NewRecorder()
    
    	handler := http.HandlerFunc(SomeHandler)
    	handler.ServeHTTP(w, req)
    
    	path := filepath.Join("testdata", "output.golden")
    	dat, _ := ioutil.ReadFile(path)  // no err for simplicity
    
    	body := w.Body.String()  // a very big body
    	if !strings.Contains(string(dat), body) {
    		t.Errorf("Golden file content is not in body! %v", body)
    	}
    }

: Beneficios:
: - Útil para usarla cuando el resultado del método es una estructura muy grande para o incómoda de revisarla manualmente.
: - Extrae todo el contenido del resultado esperado fuera del test, como en fixtures.

* 6. Test flags

    // In flags_test.go
    // Making use of currently available -test.short flag

    func TestVerySlowMethod(t *testing.T) {
    	if testing.Short() {
    		t.Skip("this test must be skipped in short mode.")
    	}

        // -- test the slow method
    }

Ejemplo:

    $ go test flags_test.go -test.short -test.v
    === RUN   TestVerySlowMethod
    --- SKIP: TestVerySlowMethod (0.00s)
    	flags_test.go:17: this test must be skipped in short mode.
    PASS
    ok  	command-line-arguments	0.008s

: Beneficios:
: - Se puede agrupar tests por algún patrón no explícito.

* 7. Test helpers

Reducir el ruido en el propósito del test debido a control de erroresy otros chequeos.

    func testURL(t *testing.T, url string) {
    	resp, err := http.Get(url)
        checkError(t, err)
    }

    func testFile(t *testing.T) func(t *testing.T) {
        err := ioutil.WriteFile(filePath, []byte("Stupid sexy flanders"), 0644)
        checkError(t, err)
        
        return func(t *testing.T) {  // this is tear down
        	err := os.Remove(filePath)
        	checkError(t, err)
        }
    }

    func TestWithExplicitPurpose(t *testing.T) {
        testURL(t, "http://www.mercadolibre.com.ar")
        defer testFile(t)(t)
    }

: Beneficios:
: - Expresa de forma más corta la intención dentro del test sin los controles de error.
: Consideraciones:
: - No se debe return errores; es mejor fallar directamente en la función misma.
: - Es conveniente retornar una función para limpieza si es necesario, para usarla con defer.
: A tener en cuenta:
: - Es elegante, pero puede no resultar obvio lo que se está haciendo.
: - La función de limpieza también debe poder fallar.

8. Benchmarks

8.1. Empezando:

    func BenchmarkSimple(b *testing.B) {
        for i := 0; i < b.N; i++ {
            if x := fmt.Sprintf("%d", 42); x != "42" {
                b.Fatalf("Unexpected string: %s", x)
            }
        }
    }

    func benchmarkFib(i int, b *testing.B) {
    	for n := 0; n < b.N; n++ {
    		Fib(i)
    	}
    }
    
    func BenchmarkFib(b *testing.B) {
    	b.Run("Fib3", func(b *testing.B) { benchmarkFib(3, b) })
    	b.Run("Fib10", func(b *testing.B) { benchmarkFib(10, b) })
    	b.Run("Fib40", func(b *testing.B) { benchmarkFib(40, b) })
    }

En ejecución:

    $ go test -bench=.
    BenchmarkSample-4   	10000000	       169 ns/op
    BenchmarkFib/Fib3-4 	100000000	        14.6 ns/op
    BenchmarkFib/Fib10-4         	 3000000	       498 ns/op
    BenchmarkFib/Fib40-4         	       2	 939975954 ns/op
    PASS
    ok  	_/Users/arios/code/mercadolibre/testing-talk/resources/8.benchmarks	8.170s    

: La prueba incrementa el valor de b.N hasta que el runner se satisface con la estabilidad del resultado o se cumple el tiempo máximo de ejecución.
: Se puede indicar que las pruebas incluyan los benchmarks, pero no al revés.
: Table driven test es una muy buena opción para comparar resultados.

*

8.2. Más opciones

    $ go test -run=XXX -bench=.  # no tests, but an oportunity to use flags!
    $ go test -bench=. -benchtime=20s

    $ go test -bench=. -benchmem
    BenchmarkSample-4   	10000000	       173 ns/op	      16 B/op	       2 allocs/op
    BenchmarkFib/Fib3-4 	100000000	        14.7 ns/op	       0 B/op	       0 allocs/op
    BenchmarkFib/Fib10-4         	 3000000	       503 ns/op	       0 B/op	       0 allocs/op
    BenchmarkFib/Fib40-4         	       2	 915035933 ns/op	       0 B/op	       0 allocs/op
    PASS
    ok  	_/Users/arios/code/mercadolibre/testing-talk/resources/8.benchmarks	8.203s

8.3. Comparación

Usando la herramienta `golang.org/x/tools/cmd/benchcmp`:

    $ go test -bench=. some-test.go > new.txt
    $ git co .
    $ go test -bench=. some-test.go > old.txt
    $ benchcmp old.txt new.txt
    benchmark                old ns/op     new ns/op     delta
    BenchmarkSample-4        170           174           +2.35%
    BenchmarkFib/Fib3-4      14.7          258           +1655.10%
    BenchmarkFib/Fib10-4     487           749           +53.80%
    BenchmarkFib/Fib40-4     947615897     908158099     -4.16%    

* Recomendaciones

* a. Limitarse a la API pùblica

- Testar sólo la API pública, a menos que la implementación sea tan compleja que amerite más.
- Los tests blackbox nos ayudan a no testear lo que no es necesario, pero debemos ser cuidadosos.

* b. Mocks: ¿son realmente necesarios?

- Algunas interfaces no valen la pena ser imitadas, sino implementadas de forma más sencilla.
- El paquete `net.Conn` es fácil 

* c. Configurabilidad

- A.k.a. estado global.
- Aún frente aquellos valores que no cambiarán jamás, es mejor exportarlos como configuraciones con valores default.
- De no ser posible, administrarlos internamente para que puedan ser sobrescritos por los test (como en `var`s).

* d. Subprocessing

- Lo ideal es ejecutar el binario y no simularlo.
- Si el binario no está disponible no tiene sentido correr el test.
- Tener en cuenta que `exec.Cmd` es un struct y puede ser reemplazado ;)

* e. Paralelización

- No es recomendado a menos que se esté completamente seguros de que un test no puede interferir en los resultados de otro.
- Preparar los test para que sean paralelizables es costoso si no se tiene en cuenta desde el principio.
